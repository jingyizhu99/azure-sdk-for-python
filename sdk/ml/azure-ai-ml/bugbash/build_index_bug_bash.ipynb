{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this example\n",
    "This sample is useful for developers and data scientists who wish to use their data to create an Index which can be used in the RAG pattern.\n",
    "\n",
    "This sample shows how to:\n",
    "- create an index locally or on the cloud with Azure AI resources\n",
    "- register a local index to cloud\n",
    "- retrieve index from the cloud\n",
    "- consume an index in langchain\n",
    "\n",
    "## Before you begin\n",
    "\n",
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project details\n",
    "subscription_id: str = \"<your-subscription-id>\"\n",
    "resource_group_name: str = \"<your-resource-group>\"\n",
    "project_name: str = \"<your-project-name>\"\n",
    "\n",
    "# connection details\n",
    "ai_search_connection_name: str = \"<your-ai-search-connection>\"\n",
    "aoai_connection_name: str = \"<your-aoai-connection>\"\n",
    "# serverless_connection_name: str = \"<your-serverless-connection>\"\n",
    "\n",
    "# names of indexes we will create\n",
    "local_index_name = \"local-index\"\n",
    "cloud_index_name = \"cloud-index\"\n",
    "\n",
    "# model used for embedding\n",
    "embedding_model_aoai: str = \"text-embedding-ada-002\"\n",
    "deployment_name_aoai: str = \"text-embedding-ada-002\"\n",
    "embedding_model_cohere: str = \"cohere-embed-v3-multilingual\" # or \"cohere-embed-v3-english\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to your project\n",
    "\n",
    "To start with let us create a config file with your project details. This file can be used in this sample or other samples to connect to your workspace. To get the required details, you can go to the Project Overview page in the AI Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "config = {\n",
    "    \"subscription_id\": subscription_id,\n",
    "    \"resource_group\": resource_group_name,\n",
    "    \"project_name\": project_name,\n",
    "}\n",
    "\n",
    "p = Path(\"config.json\")\n",
    "\n",
    "with p.open(mode=\"w\") as file:\n",
    "    file.write(json.dumps(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initalize MlClient to interact with resources in your Azure AI Studio.\n",
    "\n",
    "Please make sure you have connections for your embedding model and Azure AI Search in this workspace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "client=MLClient.from_config(DefaultAzureCredential(), path=\"./config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve connections to embedding model and AI Search\n",
    "We will use an Azure Open AI service to access the LLM and embedding model. We will also use an Azure Cognitive Search to store the index. Let us get the details of these from your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoai_connection = client.connections.get(aoai_connection_name)\n",
    "ai_search_connection = client.connections.get(ai_search_connection_name)\n",
    "# serverless_connection = client.connections.get(serverless_connection_name)\n",
    "\n",
    "print(f\"aoai connection id is {aoai_connection.id}\")\n",
    "print(f\"aoai connection id is {ai_search_connection.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Build an Index\n",
    "You can build index locally or on the cloud with Azure AI resources\n",
    "\n",
    "#### 1.1 Build index locally\n",
    "\n",
    "##### 1.1.1 Input types\n",
    "You can build index from\n",
    "1. local files or\n",
    "2. an existing ai search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.rag.resources import AzureAISearchSource, LocalSource\n",
    "# local files\n",
    "local_input_source_local = LocalSource(input_data=\"product-info/\")\n",
    "\n",
    "# existing ai search index \n",
    "# keys might be different, please refer to your MLIndex\n",
    "local_input_source_ai_search = AzureAISearchSource(ai_search_index_name=\"<index-name>\",\n",
    "                                   ai_search_content_key=\"content\",\n",
    "                                   ai_search_embedding_key=\"contentVector\",\n",
    "                                   ai_search_title_key=\"title\",\n",
    "                                   ai_search_metadata_key=\"meta_json_string\",\n",
    "                                   ai_search_connection_id=ai_search_connection.id\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.3 With AOAI embedding model\n",
    "To connect to your aoai embedding model, you can either set your api-key and endpoint in the environment variable, or pass in connction id if you have a connection to the model deployment in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.rag.resources import LocalSource, AzureAISearchConfig, EmbeddingsModelConfig\n",
    "from promptflow.rag import build_index\n",
    "\n",
    "ai_search_index_path=build_index(\n",
    "    name=local_index_name + \"aoai\",  # name of your index\n",
    "    embeddings_model_config=EmbeddingsModelConfig(\n",
    "        model_name=embedding_model_aoai,\n",
    "        deployment_name=deployment_name_aoai,\n",
    "        connection_id=aoai_connection.id\n",
    "    ),\n",
    "    input_source=LocalSource(input_data=\"product-info/\"),  # the location of your file/folders\n",
    "    index_config=AzureAISearchConfig(\n",
    "        ai_search_index_name=local_index_name + \"-aoai-store\" # the name of the index store inside the azure ai search service\n",
    "    ),\n",
    "    tokens_per_chunk = 800, # Optional field - Maximum number of tokens per chunk\n",
    "    token_overlap_across_chunks = 0, # Optional field - Number of tokens to overlap between chunks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.4 With Cohere embedding model\n",
    "To use your cohere embedding model, please specify the connection id (or connection_config) to the model deployment you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.rag.resources import LocalSource, AzureAISearchConfig, EmbeddingsModelConfig, ConnectionConfig\n",
    "from promptflow.rag import build_index\n",
    "\n",
    "ai_search_index_path=build_index(\n",
    "    name=local_index_name + \"cohere\",  # name of your index\n",
    "    embeddings_model_config=EmbeddingsModelConfig(\n",
    "        model_name=embedding_model_cohere,\n",
    "        connection_id=serverless_connection.id\n",
    "        # connection_config=ConnectionConfig(\n",
    "        #     subscription = \"<subscription>\",\n",
    "        #     resource_group = \"<resource-group>\",\n",
    "        #     workspace = \"<workspace>\",\n",
    "        #     connection_name = \"<connection-name>\"\n",
    "        # )\n",
    "    ),\n",
    "    input_source=LocalSource(input_data=\"product-info/\"),  # the location of your file/folders\n",
    "    index_config=AzureAISearchConfig(\n",
    "        ai_search_index_name=local_index_name + \"cohere-store\" # the name of the index store inside the azure ai search service\n",
    "    ),\n",
    "    tokens_per_chunk = 800, # Optional field - Maximum number of tokens per chunk\n",
    "    token_overlap_across_chunks = 0, # Optional field - Number of tokens to overlap between chunks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2 Register the index\n",
    "Register the index so that it shows up in the AI Studio Project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from azure.ai.ml.entities import Index\n",
    "client.indexes.create_or_update(Index(name=local_index_name, path=ai_search_index_path, version=\"1\", stage=\"Development\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Build index on cloud\n",
    "\n",
    "##### 1.2.1 Input types\n",
    "You can build index from the following four types of inputs:\n",
    "1. Local files/folders\n",
    "2. Github repo\n",
    "3. Azure Storages\n",
    "4. Existing AI Search index\n",
    "\n",
    "Examples of various data sources:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input sources\n",
    "from azure.ai.ml.entities._indexes import LocalSource, AISearchSource, GitSource\n",
    "\n",
    "# Local source\n",
    "input_source_local = LocalSource(input_data=\"product-info/\")\n",
    "\n",
    "# Github repo\n",
    "input_source_git = GitSource(git_url=\"https://github.com/rust-lang/book.git\", git_branch_name=\"main\", git_connection_id=\"\")\n",
    "\n",
    "# Azure Storage\n",
    "input_source_subscription = \"<subscription>\"\n",
    "input_source_resource_group = \"<resource_group>\"\n",
    "input_source_workspace = \"<workspace>\"\n",
    "input_source_datastore = \"<datastore_name>\"\n",
    "input_source_path = \"path\"\n",
    "input_source_urls=f\"azureml://subscriptions/{input_source_subscription}/resourcegroups/{input_source_resource_group}/workspaces/{input_source_workspace}/datastores/{input_source_datastore}/paths/{input_source_path}\"\n",
    "\n",
    "# Existing AI Search index\n",
    "input_source_ai_search = AISearchSource(ai_search_index_name=\"remote_index\",\n",
    "                                        ai_search_index_content_key=\"content\",\n",
    "                                        ai_search_index_embedding_key=\"contentVector\",\n",
    "                                        ai_search_index_title_key=\"title\",\n",
    "                                        ai_search_index_metadata_key=\"meta_json_string\",\n",
    "                                        ai_search_index_connection_id=ai_search_connection.id\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2 Connections\n",
    "\n",
    "The following connection types are supported:\n",
    "1. Entra id connections\n",
    "2. Api key based connections\n",
    "3. Connections to serverless models (cohere)\n",
    "\n",
    "Please make sure you have deployments for the embedding model in this workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities._indexes import ModelConfiguration\n",
    "## aoai and acs connections\n",
    "aoai_connection = client.connections.get(\"<aoai_connection>\", populate_secrets=True)\n",
    "ai_search_connection = client.connections.get(\"<search_connection>\")\n",
    "embeddings_model_config = ModelConfiguration.from_connection(aoai_connection, \n",
    "                                                             model_name=\"text-embedding-ada-002\",\n",
    "                                                             deployment_name=\"text-embedding-ada-002\")\n",
    "# workaround for connections.get() not returning api_key\n",
    "# os.environ[\"AZURE_OPENAI_API_KEY\"] = \"<aoai_api_key>\"\n",
    "\n",
    "## aoai and acs connections - entra id\n",
    "## TODO: You will hit embedding error with aoai entra-id connection, fix in progress\n",
    "# aoai_connection = client.connections.get(\"<aoai_entra_id_connection_name>\")\n",
    "# ai_search_connection = client.connections.get(\"<search_entra_id_connection_name>>\")\n",
    "# embeddings_model_config = ModelConfiguration.from_connection(aoai_connection, \n",
    "#                                                              model_name=\"text-embedding-ada-002\",\n",
    "#                                                              deployment_name=\"text-embedding-ada-002\")\n",
    "\n",
    "## cohere embedding model\n",
    "# embeddings_model_config = ModelConfiguration.from_connection(serverless_connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3 Build index on cloud\n",
    "\n",
    "You can change the input_source to anything listed above. input_source_credential is needed for Azure Storage input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities._credentials import UserIdentityConfiguration\n",
    "from azure.ai.ml.entities._indexes import AzureAISearchConfig\n",
    "\n",
    "client.indexes.build_index(\n",
    "    name=cloud_index_name, # name of your index\n",
    "    embeddings_model_config=embeddings_model_config,\n",
    "    input_source=input_source_local, \n",
    "    # input_source_credential=UserIdentityConfiguration(), # user specified identity used to access the data.\n",
    "    index_config=AzureAISearchConfig(\n",
    "        ai_search_index_name=cloud_index_name,  # the name of the index store in AI search service\n",
    "        ai_search_connection_id=ai_search_connection.id, # AI Search connection details\n",
    "    ),\n",
    "    tokens_per_chunk = 800, # Optional field - Maximum number of tokens per chunk\n",
    "    token_overlap_across_chunks = 0, # Optional field - Number of tokens to overlap between chunks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Retrieve index from the cloud\n",
    "Get the index object once the job is finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_index=client.indexes.get(name=cloud_index_name, label=\"latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Consume the index as a langchain retriever\n",
    "\n",
    "Known issue this is broken for index build on cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever = ml_index.as_langchain_retriever()\n",
    "# retriever.get_relevant_documents(\"which tent is the most waterproof?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workaround: specify the storage uri of the MLIndex file and consume it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"azureml://subscriptions/f375b912-331c-4fc5-8e9f-2d7205e3e036/resourcegroups/rg-jingyizhuai/workspaces/jingyizhu-project-2/datastores/workspaceblobstore/paths/indexes/remote-local-02/3a76509a-600b-4c65-a593-1b0944fa68ff/\"\n",
    "from azureml.rag.mlindex import MLIndex as InternalMLIndex\n",
    "retriever = InternalMLIndex(str(path)).as_langchain_retriever()\n",
    "retriever.get_relevant_documents(\"which tent is the most waterproof?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_index=client.indexes.get(name=cloud_index_name, label=\"latest\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
